{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "from semantic_router.splitters import RollingWindowSplitter\n",
    "from semantic_router.utils.logger import logger\n",
    "from transcriptTools import (\n",
    "    getTranscriptFiles,\n",
    "    transcript_to_srt,\n",
    "    reindex,\n",
    "    srt_to_transcript,\n",
    ")\n",
    "\n",
    "ROOT = os.getcwd()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logger.setLevel(\"WARNING\")  # reduce logs from splitter\n",
    "\n",
    "encoder = OpenAIEncoder(name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40186/2090129127.py:27: UserWarning: Splitters are being deprecated. They have moved to their own package. Please migrate to the `semantic-chunkers` package. More information can be found at:\n",
      "https://github.com/aurelio-labs/semantic-chunkers\n",
      "  splitter = RollingWindowSplitter(\n"
     ]
    }
   ],
   "source": [
    "transcriptDir = os.path.join(ROOT, \"labeled\")\n",
    "files = getTranscriptFiles(transcriptDir)\n",
    "\n",
    "truncatedDir = os.path.join(ROOT, \"truncated\")\n",
    "os.makedirs(truncatedDir, exist_ok=True)\n",
    "os.makedirs(os.path.join(truncatedDir, \"rotl\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(truncatedDir, \"roadwork\"), exist_ok=True)\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    filepath, showname, filename = file\n",
    "    outpath = os.path.join(truncatedDir, showname, filename)\n",
    "    if showname == \"roadwork\":\n",
    "        shutil.copyfile(filepath, outpath)\n",
    "    else:\n",
    "        try:\n",
    "            transcript = srt_to_transcript(filepath)\n",
    "            filtered = [\n",
    "                (idx, start, end, speaker, speech)\n",
    "                for idx, start, end, speaker, speech in transcript\n",
    "                if speech != \"\"\n",
    "            ]\n",
    "            reindexed = reindex(filtered)\n",
    "            content_with_speaker = [\n",
    "                speech for idx, start, end, speaker, speech in reindexed\n",
    "            ]\n",
    "            splitter = RollingWindowSplitter(\n",
    "                encoder=encoder,\n",
    "                dynamic_threshold=True,\n",
    "                min_split_tokens=30,\n",
    "                max_split_tokens=500,\n",
    "                window_size=2,\n",
    "                plot_splits=False,  # set this to true to visualize chunking\n",
    "                enable_statistics=False,  # to print chunking stats\n",
    "            )\n",
    "\n",
    "            splits = splitter(content_with_speaker)\n",
    "            chunks = [split.docs for split in splits]\n",
    "            first_chunk_length = len(chunks[0])\n",
    "            truncated_transcript = [\n",
    "                (idx, start, end, speaker, speech)\n",
    "                for idx, start, end, speaker, speech in reindexed\n",
    "                if int(idx) > first_chunk_length\n",
    "            ]\n",
    "            srt = transcript_to_srt(truncated_transcript)\n",
    "            f = open(outpath, \"w\")\n",
    "            f.write(srt)\n",
    "            f.close()\n",
    "        except Exception as error:\n",
    "            print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40186/309004163.py:17: UserWarning: Splitters are being deprecated. They have moved to their own package. Please migrate to the `semantic-chunkers` package. More information can be found at:\n",
      "https://github.com/aurelio-labs/semantic-chunkers\n",
      "  splitter = RollingWindowSplitter(\n"
     ]
    }
   ],
   "source": [
    "truncatedDir = os.path.join(ROOT, \"truncated\")\n",
    "chunkedDir = os.path.join(ROOT, \"chunked\")\n",
    "\n",
    "os.makedirs(chunkedDir, exist_ok=True)\n",
    "os.makedirs(os.path.join(chunkedDir, \"rotl\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(chunkedDir, \"roadwork\"), exist_ok=True)\n",
    "\n",
    "files = getTranscriptFiles(truncatedDir)\n",
    "\n",
    "for file in files:\n",
    "    filepath, showname, filename = file\n",
    "    outpath = os.path.join(chunkedDir, showname, filename)\n",
    "    transcript = srt_to_transcript(filepath)\n",
    "    content_with_speaker = [\n",
    "        f\"{speaker}: {speech} \" for idx, start, end, speaker, speech in transcript\n",
    "    ]\n",
    "    splitter = RollingWindowSplitter(\n",
    "        encoder=encoder,\n",
    "        dynamic_threshold=True,\n",
    "        min_split_tokens=100,\n",
    "        max_split_tokens=500,\n",
    "        window_size=2,\n",
    "        plot_splits=False,  # set this to true to visualize chunking\n",
    "        enable_statistics=False,  # to print chunking stats\n",
    "    )\n",
    "\n",
    "    splits = splitter(content_with_speaker)\n",
    "    chunks = [\"\\n\".join(split.docs) for split in splits]\n",
    "    text = \"\\n\\n\".join(chunks)\n",
    "    outpath = os.path.join(ROOT, \"chunked\", showname, filename.replace(\".srt\", \".txt\"))\n",
    "    f = open(outpath, \"w\")\n",
    "    f.write(text)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
